# -*- coding: utf-8 -*-
"""Detectron2 Tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5

# Detectron2 Beginner's Tutorial

<img src="https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png" width="500">

Welcome to detectron2! This is the official colab tutorial of detectron2. Here, we will go through some basics usage of
detectron2, including the following:

* Run inference on images or videos, with an existing detectron2 model
* Train a detectron2 model on a new dataset

You can make a copy of this tutorial by "File -> Open in playground mode" and make changes there. __DO NOT__ request
access to this tutorial.
"""

import json
import os
import random
import re
from glob import glob
from typing import List

import cv2
import detectron2
import numpy as np
import torch
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2.engine import DefaultPredictor
from detectron2.engine import DefaultTrainer
from detectron2.structures import BoxMode, polygons_to_bitmask
from detectron2.utils.logger import setup_logger
from detectron2.utils.visualizer import Visualizer
from pycocotools import mask as mask_utils
from tqdm import tqdm

from cfg import cfg

TORCH_VERSION = ".".join(torch.__version__.split(".")[:2])
CUDA_VERSION = torch.__version__.split("+")[-1]
print("torch: ", TORCH_VERSION, "; cuda: ", CUDA_VERSION)
print("detectron2:", detectron2.__version__)

# Some basic setup:
# Setup detectron2 logger
setup_logger()

# """# Run a pre-trained detectron2 model
#
# We first download an image from the COCO dataset:
# """
#
# # !wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg
# example_im = cv2.imread("./data/neurofinder.00.00/images/image00000.tiff")
# cv2.imshow("here's what we're working with", example_im)
# cv2.waitKey(0)
# cv2.destroyAllWindows()
#
# """Then, we create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on this image."""
#
# test_cfg = get_cfg()
# # add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library
# test_cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
# test_cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
# # Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well
# test_cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
# predictor = DefaultPredictor(test_cfg)
# outputs = predictor(example_im)
#
# # look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification
# print(outputs["instances"].pred_classes)
# print(outputs["instances"].pred_boxes)
#
# # We can use `Visualizer` to draw the predictions on the image.
# v = Visualizer(example_im[:, :, ::-1], MetadataCatalog.get(test_cfg.DATASETS.TRAIN[0]), scale=1.2)
# out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
# cv2.imshow("untrained detection (should not find anything)", out.get_image()[:, :, ::-1])
# cv2.waitKey(0)
# cv2.destroyAllWindows()

"""Register the neuron dataset to detectron2, following the
[detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html).
Here, the dataset is in its custom format, therefore we write a function to parse it and prepare it into COCO
standard format. User should write such a function when using a dataset in custom format. 
"""

# if your dataset is already in COCO format, this can be replaced by the following three lines:
# from detectron2.data.datasets import register_coco_instances
# register_coco_instances("my_dataset_train", {}, "json_annotation_train.json", "path/to/image/dir")
# register_coco_instances("my_dataset_val", {}, "json_annotation_val.json", "path/to/image/dir")


def polygon_to_mask(polygon: List[List], shape):
    """
    polygon: a list of [[x1, y1], [x2, y2],....]
    shape: shape of bitmask
    Return: RLE type of mask
    source: https://www.kaggle.com/code/linrds/convert-rle-to-polygons
    """
    polygon = [x_or_y for coords in polygon for x_or_y in coords]
    return polygons_to_bitmask([np.asarray(polygon) + 0.25], shape[0], shape[1])


def polygon_to_rle(polygon: List, shape):
    m = polygon_to_mask(polygon, shape)
    return mask_utils.encode(np.asfortranarray(m))


def bounding_box(points):
    """ return the bounding box of a set of points"""
    return np.array([*np.min(points, axis=0), *np.max(points, axis=0)])


def region_info(region: List, dims: List):
    assert len(region) > 2, f"coordinate list for a polygon must have at least 3 points: {region}"
    return {
        "mask": polygon_to_mask(region, dims),
        "bbox": bounding_box(region),
        "rle": polygon_to_rle(region, dims),
        "polygon": [[x_or_y + 0.5 for coords in region for x_or_y in coords]],
    }


def get_neuron_dicts(data_dir):  # contains e.g. neurofinder.00.00/
    dataset_dicts = []
    cutoff = 1.4  # todo: so arbitrary! can we get this value from the image somehow?
    for plate_dir in glob(os.path.join(data_dir, "neurofinder*")):
        plate_num = int(re.sub(r"[^0-9]+", "", plate_dir))
        img_files = sorted(glob(os.path.join(plate_dir, "images", "*.tiff")))[:40]
        dims = cv2.imread(img_files[0]).shape
        assert dims, "could not load images from specified directory (should be e.g. neurofinder.00.00)"

        # the regions are from across the _entire_ time-series
        annot_file = os.path.join(plate_dir, "regions", "regions.json")
        with open(annot_file) as f:
            # list of {id: int, coordinates: List[List[int, int]]}.
            # id is arbitrary. coordinates describe the neuron's polygon boundary.
            regions = [region_info(s["coordinates"], dims) for s in json.load(f)]

        with tqdm(total=len(img_files)) as pbar:
            for img in img_files:
                img_num = re.sub(r"[^0-9]+", "", img)
                img_num = plate_num * (10 ** len(img_num)) + int(img_num)  # they're nicely zero-padded
                img_data = cv2.imread(img)
                annotations = [
                    {
                        "image_id": img_num,
                        "mask": info["mask"],
                        "bbox": info["bbox"],
                        "bbox_mode": BoxMode.XYXY_ABS,
                        "segmentation": info["polygon"],
                        "category_id": 0,
                        "iscrowd": False,
                    }
                    for info in regions
                    if np.mean(img_data[info["mask"]]) > cutoff
                ]

                dataset_dicts.append(
                    {"id": img_num, "file_name": img, "height": dims[0], "width": dims[1], "annotations": annotations}
                )
                pbar.update(1)

    return dataset_dicts


DatasetCatalog.register("neuron_train", lambda x="train": get_neuron_dicts("data"))
MetadataCatalog.get("neuron_train").set(thing_classes=["neuron"])
# DatasetCatalog.register("neuron_val", lambda x="val": get_neuron_dicts("data/val"))
# MetadataCatalog.get("neuron_val").set(thing_classes=["neuron"])

neuron_metadata = MetadataCatalog.get("neuron_train")


if __name__ == "__main__":
    """To verify the dataset is in correct format, let's visualize the annotations of randomly selected samples in the
    training set:
    """
    training_dicts = get_neuron_dicts("data")
    for d in random.sample(training_dicts, 2):
        img = cv2.imread(d["file_name"])
        visualizer = Visualizer(img[:, :, ::-1], metadata=neuron_metadata, scale=0.5)
        out = visualizer.draw_dataset_dict(d)
        cv2.imshow("training data", img)
        cv2.imshow("annotations (do you see corresponding smudges?)", out.get_image()[:, :, ::-1])
        cv2.waitKey(0)
        cv2.destroyAllWindows()

    """## Train!

    Now, let's fine-tune a COCO-pretrained R50-FPN Mask R-CNN model on the neuron dataset. It takes ~2 minutes to train 300
    iterations on a P100 GPU.
    """
    trainer = DefaultTrainer(cfg)
    trainer.resume_or_load(resume=False)
    trainer.train()

    # Look at training curves in tensorboard?
    # %load_ext tensorboard
    # %tensorboard --logdir output
