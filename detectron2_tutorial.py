# -*- coding: utf-8 -*-
"""Detectron2 Tutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5

# Detectron2 Beginner's Tutorial

<img src="https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png" width="500">

Welcome to detectron2! This is the official colab tutorial of detectron2. Here, we will go through some basics usage of
detectron2, including the following:

* Run inference on images or videos, with an existing detectron2 model
* Train a detectron2 model on a new dataset

You can make a copy of this tutorial by "File -> Open in playground mode" and make changes there. __DO NOT__ request
access to this tutorial.
"""

import json
import os
import random
from glob import glob
from typing import List

import cv2
import detectron2
import numpy as np
import torch
from detectron2 import model_zoo
from detectron2.config import get_cfg
from detectron2.data import MetadataCatalog, DatasetCatalog
from detectron2.data import build_detection_test_loader
from detectron2.engine import DefaultPredictor
from detectron2.engine import DefaultTrainer
from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.structures import BoxMode, polygons_to_bitmask
from detectron2.utils.logger import setup_logger
from detectron2.utils.visualizer import ColorMode
from detectron2.utils.visualizer import Visualizer
from pycocotools import mask as mask_utils
from tqdm import tqdm

TORCH_VERSION = ".".join(torch.__version__.split(".")[:2])
CUDA_VERSION = torch.__version__.split("+")[-1]
print("torch: ", TORCH_VERSION, "; cuda: ", CUDA_VERSION)
print("detectron2:", detectron2.__version__)

# Some basic setup:
# Setup detectron2 logger
setup_logger()

"""# Run a pre-trained detectron2 model

We first download an image from the COCO dataset:
"""

# !wget http://images.cocodataset.org/val2017/000000439715.jpg -q -O input.jpg
example_im = cv2.imread("./data/neurofinder.00.00/images/image00000.tiff")
cv2.imshow("example image", example_im)

"""Then, we create a detectron2 config and a detectron2 `DefaultPredictor` to run inference on this image."""

cfg = get_cfg()
# add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library
cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
# Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
predictor = DefaultPredictor(cfg)
outputs = predictor(example_im)

# look at the outputs. See https://detectron2.readthedocs.io/tutorials/models.html#model-output-format for specification
print(outputs["instances"].pred_classes)
print(outputs["instances"].pred_boxes)

# We can use `Visualizer` to draw the predictions on the image.
v = Visualizer(example_im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2.imshow("detected objects", out.get_image()[:, :, ::-1])

"""Register the neuron dataset to detectron2, following the
[detectron2 custom dataset tutorial](https://detectron2.readthedocs.io/tutorials/datasets.html).
Here, the dataset is in its custom format, therefore we write a function to parse it and prepare it into COCO
standard format. User should write such a function when using a dataset in custom format. 
"""

# if your dataset is already in COCO format, this can be replaced by the following three lines:
# from detectron2.data.datasets import register_coco_instances
# register_coco_instances("my_dataset_train", {}, "json_annotation_train.json", "path/to/image/dir")
# register_coco_instances("my_dataset_val", {}, "json_annotation_val.json", "path/to/image/dir")


def polygon_to_mask(polygon: List[List], shape):
    """
    polygon: a list of [[x1, y1], [x2, y2],....]
    shape: shape of bitmask
    Return: RLE type of mask
    source: https://www.kaggle.com/code/linrds/convert-rle-to-polygons
    """
    polygon = [x_or_y for coords in polygon for x_or_y in coords]
    return polygons_to_bitmask([np.asarray(polygon) + 0.25], shape[0], shape[1])


def polygon_to_rle(polygon: List, shape):
    m = polygon_to_mask(polygon, shape)
    return mask_utils.encode(np.asfortranarray(m))


def bounding_box(points):
    """ return the bounding box of a set of points"""
    return np.array([*np.min(points, axis=0), *np.max(points, axis=0)])


def region_info(region: List, dims: List):
    return {
        "mask": polygon_to_mask(region, dims),
        "bbox": bounding_box(region),
        "rle": polygon_to_rle(region, dims),
        "polygon": region,
    }


def get_neuron_dicts(data_dir):  # contains e.g. neurofinder.00.00/
    dataset_dicts = []
    cutoff = 1.4  # todo: so arbitrary! can we get this value from the image somehow?
    for plate_dir in glob(os.path.join(data_dir, "neurofinder*")):
        img_files = sorted(glob(os.path.join(plate_dir, "images", "*.tiff")))[:20]
        dims = cv2.imread(img_files[0]).shape
        assert dims, "could not load images from specified directory (should be e.g. neurofinder.00.00)"

        # the regions are from across the _entire_ time-series
        annot_file = os.path.join(plate_dir, "regions", "regions.json")
        with open(annot_file) as f:
            # list of {id: int, coordinates: List[List[int, int]]}.
            # id is arbitrary. coordinates describe the neuron's polygon boundary.
            regions = [region_info(s["coordinates"], dims) for s in json.load(f)]

        with tqdm(total=len(img_files)) as pbar:
            for img in img_files:
                img_data = cv2.imread(img)
                annotations = []
                for info in regions:
                    if np.mean(img_data[info["mask"]]) > cutoff:
                        annotations.append({
                            "mask": info["mask"],
                            "bbox": info["bbox"],
                            "bbox_mode": BoxMode.XYXY_ABS,
                            "segmentation": info["polygon"],
                            "category_id": 0,
                            "iscrowd": False,  # todo: what does this mean?
                        })

                    # which regions apply to this image

                dataset_dicts.append({
                    "image_id": img,
                    "file_name": img,
                    "height": dims[0],
                    "width": dims[1],
                    "annotations": annotations,
                })
                pbar.update(1)

    return dataset_dicts


d = "train"  # todo "val"
DatasetCatalog.register(f"neuron_{d}", lambda x=d: get_neuron_dicts("data"))
MetadataCatalog.get(f"neuron_{d}").set(thing_classes=["neuron"])
neuron_metadata = MetadataCatalog.get("neuron_train")


"""To verify the dataset is in correct format, let's visualize the annotations of randomly selected samples in the
training set:


"""

# todo for simpler verification, put this into its own script, and just have the work up until now save to a file.
training_dicts = get_neuron_dicts("data")
for d in random.sample(training_dicts, 3):
    img = cv2.imread(d["file_name"])
    visualizer = Visualizer(img[:, :, ::-1], metadata=neuron_metadata, scale=0.5)
    out = visualizer.draw_dataset_dict(d)
    cv2.imshow("random output", out.get_image()[:, :, ::-1])


"""## Train!

Now, let's fine-tune a COCO-pretrained R50-FPN Mask R-CNN model on the neuron dataset. It takes ~2 minutes to train 300
iterations on a P100 GPU.

"""


cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
cfg.DATASETS.TRAIN = ("neuron_train",)
cfg.DATASETS.TEST = ()
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(
    "COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"
)  # Let training initialize from model zoo
cfg.SOLVER.IMS_PER_BATCH = 2  # This is the real "batch size" commonly known to deep learning people
cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR
cfg.SOLVER.MAX_ITER = (
    300  # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset
)
cfg.SOLVER.STEPS = []  # do not decay learning rate
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = (
    128  # The "RoIHead batch size". 128 is faster, and good enough for this toy dataset (default: 512)
)
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (neuron).
# (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)
# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses
# num_classes+1 here.

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = DefaultTrainer(cfg)
trainer.resume_or_load(resume=False)
trainer.train()

# Commented out IPython magic to ensure Python compatibility.
# Look at training curves in tensorboard:
# %load_ext tensorboard
# %tensorboard --logdir output

"""## Inference & evaluation using the trained model
Now, let's run inference with the trained model on the neuron validation dataset. First, let's create a predictor using
the model we just trained:


"""

# Inference should use the config with parameters that are used in training
# cfg now already contains everything we've set previously. We changed it a bit for inference:
cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, "model_final.pth")  # path to the model we just trained
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set a custom testing threshold
predictor = DefaultPredictor(cfg)

"""Then, we randomly select several samples to visualize the prediction results."""

val_dicts = get_neuron_dicts("neuron/val")
for d in random.sample(val_dicts, 3):
    example_im = cv2.imread(d["file_name"])
    outputs = predictor(
        example_im
    )  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format
    v = Visualizer(
        example_im[:, :, ::-1],
        metadata=neuron_metadata,
        scale=0.5,
        instance_mode=ColorMode.IMAGE_BW,  # remove the colors of unsegmented pixels. segmentation models only.
    )
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
    cv2.imshow("window name", out.get_image()[:, :, ::-1])

"""We can also evaluate its performance using AP metric implemented in COCO API.
This gives an AP of ~70. Not bad!
"""

evaluator = COCOEvaluator("neuron_val", output_dir="./output")
val_loader = build_detection_test_loader(cfg, "neuron_val")
print(inference_on_dataset(predictor.model, val_loader, evaluator))
# another equivalent way to evaluate the model is to use `trainer.test`

"""# Other types of builtin models

We showcase simple demos of other types of models below:
"""

# Inference with a keypoint detection model
cfg = get_cfg()  # get a fresh new config
cfg.merge_from_file(model_zoo.get_config_file("COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml"))
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set threshold for this model
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml")
predictor = DefaultPredictor(cfg)
outputs = predictor(example_im)
v = Visualizer(example_im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
out = v.draw_instance_predictions(outputs["instances"].to("cpu"))
cv2.imshow("keypoints what?", out.get_image()[:, :, ::-1])

# Inference with a panoptic segmentation model
cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml"))
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-PanopticSegmentation/panoptic_fpn_R_101_3x.yaml")
predictor = DefaultPredictor(cfg)
panoptic_seg, segments_info = predictor(example_im)["panoptic_seg"]
v = Visualizer(example_im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)
out = v.draw_panoptic_seg_predictions(panoptic_seg.to("cpu"), segments_info)
cv2.imshow("segmentation what?", out.get_image()[:, :, ::-1])
